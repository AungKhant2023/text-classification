{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8baf75c1",
      "metadata": {
        "id": "8baf75c1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "from tensorflow.keras.layers import Embedding, Layer, Dense, Dropout, MultiHeadAttention, LayerNormalization, Input, GlobalAveragePooling1D\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uKyYtPBnofsb",
        "outputId": "92f90fb6-f3f9-4527-adba-53b3c0db98bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uKyYtPBnofsb",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4345270e",
      "metadata": {
        "id": "4345270e",
        "outputId": "782537a7-5622-4fdd-d0ee-3bafce061c4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stop_words.remove('not')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "class MyanmarTextPreprocessor():\n",
        "    def __init__(self, dict_path: str, stop_path: str):\n",
        "        self.dictionary = self.load_dictionary(dict_path)\n",
        "        self.stopwords = self.load_stopwords(stop_path)\n",
        "\n",
        "    # Load dictionary into a set\n",
        "    def load_dictionary(self, dict_path):\n",
        "        dictionary = set()\n",
        "        with open(dict_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word = line.strip()\n",
        "                if word:\n",
        "                    dictionary.add(word)\n",
        "        return dictionary\n",
        "\n",
        "    # Load stopwords into a set\n",
        "    def load_stopwords(self, stopword_path):\n",
        "        stopwords = set()\n",
        "        with open(stopword_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                word = line.strip()\n",
        "                if word:\n",
        "                    stopwords.add(word)\n",
        "        return stopwords\n",
        "\n",
        "    # Merge syllables based on dictionary\n",
        "    def merge_with_dictionary(self, syllables):\n",
        "        merged_tokens = []\n",
        "        i = 0\n",
        "        while i < len(syllables):\n",
        "            matched = False\n",
        "            for j in range(len(syllables), i, -1):\n",
        "                combined = ''.join(syllables[i:j])\n",
        "                if combined in self.dictionary:\n",
        "                    merged_tokens.append(combined)\n",
        "                    i = j\n",
        "                    matched = True\n",
        "                    break\n",
        "            if not matched:\n",
        "                merged_tokens.append(syllables[i])\n",
        "                i += 1\n",
        "        return merged_tokens\n",
        "\n",
        "    def preprocessing(self, text: str):\n",
        "        text = re.sub(r\"(([A-Za-z0-9]+)|[က-အ|ဥ|ဦ](င်္|[က-အ][ှ]*[့း]*[်]|္[က-အ]|[ါ-ှႏꩻ][ꩻ]*){0,}|.)\", r\"\\1 \", text)\n",
        "        text = text.strip().split()\n",
        "        merged_tokens = self.merge_with_dictionary(text)\n",
        "        filtered_tokens = [token for token in merged_tokens if token not in self.stopwords]\n",
        "        return ' '.join(filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = MyanmarTextPreprocessor('/content/drive/MyDrive/Datasets/dict-words.txt', '/content/drive/MyDrive/Datasets/sw.txt')\n"
      ],
      "metadata": {
        "id": "BIyl1TtTnWPR"
      },
      "id": "BIyl1TtTnWPR",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = df['Sentence']\n",
        "label = df['Label']\n",
        "X_train, X_val, y_train, y_val = train_test_split(sentences, label, test_size=0.2, stratify=label, random_state=40)"
      ],
      "metadata": {
        "id": "Ir6KTCpwiP6N"
      },
      "id": "Ir6KTCpwiP6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_clean = X_train.apply(preprocessor.preprocessing)\n",
        "X_val_clean = X_val.apply(preprocessor.preprocessing)"
      ],
      "metadata": {
        "id": "Opuy2iCtiTC6"
      },
      "id": "Opuy2iCtiTC6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overlap = set(X_train_clean).intersection(set(X_val_clean))\n",
        "print(f\"Number of overlapping samples: {len(overlap)}\")\n",
        "X_val_clean = [x for x in X_val_clean if x not in X_train_clean]"
      ],
      "metadata": {
        "id": "1QuU5M4CiWfs"
      },
      "id": "1QuU5M4CiWfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "map_label = {\n",
        "    'Social': 0,\n",
        "    'Entertainment': 1,\n",
        "    'Product&Service': 2,\n",
        "    'Business': 3,\n",
        "    'Sports': 4,\n",
        "    'Science&Technology': 5,\n",
        "    'Education': 6,\n",
        "    'Culture&History': 7,\n",
        "    'Health': 8,\n",
        "    'Environmental': 9,\n",
        "    'Political': 10,\n",
        "    'Gambling': 11,\n",
        "    'Adult Content': 12,\n",
        "}\n",
        "y_train_encoded = y_train.map(map_label)\n",
        "y_val_encoded = y_val.map(map_label)"
      ],
      "metadata": {
        "id": "-f8pnQwZjMXC"
      },
      "id": "-f8pnQwZjMXC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, heads, neurons, dropout_rate=0.5,**kwargs):\n",
        "        super(TransformerEncoder, self).__init__(**kwargs)\n",
        "        self.att = layers.MultiHeadAttention(num_heads=heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential([\n",
        "            layers.Dense(neurons, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, inputs, mask=None, training=False):\n",
        "        # Multi-head self-attention with mask\n",
        "        attn_output = self.att(inputs, inputs, attention_mask=mask)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Token + Position Embedding\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim,**kwargs):\n",
        "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "G0m5kvEhni_v"
      },
      "id": "G0m5kvEhni_v",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 100\n",
        "model = load_model('/content/drive/MyDrive/Datasets/my_transformer_model_1.h5', custom_objects={\n",
        "    'TransformerEncoder': TransformerEncoder,\n",
        "    'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
        "})\n",
        "with open('/content/drive/MyDrive/Datasets/tokenizer_1.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "oocTquVSnufj",
        "outputId": "3712cc6f-dff9-40a7-a062-151e11b1f962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "oocTquVSnufj",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train_pad,y_train_encoded,\n",
        "                    validation_data=(X_val_pad,y_val_encoded),\n",
        "                    epochs=25,\n",
        "                    batch_size=32,\n",
        "                    callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "Htx_avsirebf"
      },
      "id": "Htx_avsirebf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(X_val_pad, y_val_encoded, batch_size=32)\n",
        "print(f\"Validation Accuracy: {val_acc:.4f}\")"
      ],
      "metadata": {
        "id": "CJUdhp-Srmcy"
      },
      "id": "CJUdhp-Srmcy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred_prob = model.predict(X_val_pad)          # Predict probabilities\n",
        "y_pred = y_pred_prob.argmax(axis=1)             # Convert to predicted class indices\n",
        "y_true = y_val_encoded\n",
        "print(classification_report(y_true, y_pred, digits=4))"
      ],
      "metadata": {
        "id": "YKIWaF4hrpYM"
      },
      "id": "YKIWaF4hrpYM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('my_transformer_model3.keras')\n",
        "with open('tokenizer3.pkl', 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)"
      ],
      "metadata": {
        "id": "0DKHqaTLbD6r"
      },
      "id": "0DKHqaTLbD6r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}